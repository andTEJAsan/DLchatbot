{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tejas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from chatbot import bag_of_words, tokenize, stem\n",
    "from chatbot import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json', 'r') as f:\n",
    "\tintents = json.load(f)\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "# looping through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "\ttag = intent['tag']\n",
    "\ttags.append(tag)\n",
    "\tfor pattern in intent['patterns']:\n",
    "\t\t# tokenize each word in the sentence pattern\n",
    "\t\t# append to our list of words\n",
    "\t\tw = tokenize(pattern)\n",
    "\t\tall_words.extend(w)\n",
    "\t\txy.append((w, tag))\n",
    "ignore_words = set(['?', '.', '!'])\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence,tag) in xy:\n",
    "\tbag = bag_of_words(pattern_sentence, all_words)\n",
    "\t#PyTorch cross entropic loss needs only class labels, not one-hot\n",
    "\tlabel = tags.index(tag)\n",
    "\tX_train.append(bag)\n",
    "\ty_train.append(label)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "EPOCHS = 5000\n",
    "batch_size = 8\n",
    "lr = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "class chatDataset(Dataset):\n",
    "\tdef __init__(self):\n",
    "\t\tself.n_samples = len(X_train)\n",
    "\t\tself.x_data = X_train\n",
    "\t\tself.y_data = y_train\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.x_data[index], self.y_data[index]\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n_samples\n",
    "dataset = chatDataset()\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size,shuffle=True, num_workers=0)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 2.2127\n",
      "Epoch [20/1000], Loss: 1.0162\n",
      "Epoch [30/1000], Loss: 1.6551\n",
      "Epoch [40/1000], Loss: 1.3703\n",
      "Epoch [50/1000], Loss: 1.4050\n",
      "Epoch [60/1000], Loss: 0.0123\n",
      "Epoch [70/1000], Loss: 0.9023\n",
      "Epoch [80/1000], Loss: 0.5893\n",
      "Epoch [90/1000], Loss: 0.1970\n",
      "Epoch [100/1000], Loss: 0.0962\n",
      "Epoch [110/1000], Loss: 0.0060\n",
      "Epoch [120/1000], Loss: 0.0472\n",
      "Epoch [130/1000], Loss: 0.0062\n",
      "Epoch [140/1000], Loss: 0.4840\n",
      "Epoch [150/1000], Loss: 0.0708\n",
      "Epoch [160/1000], Loss: 0.1445\n",
      "Epoch [170/1000], Loss: 0.0029\n",
      "Epoch [180/1000], Loss: 0.0254\n",
      "Epoch [190/1000], Loss: 0.0126\n",
      "Epoch [200/1000], Loss: 0.0004\n",
      "Epoch [210/1000], Loss: 0.2283\n",
      "Epoch [220/1000], Loss: 0.0034\n",
      "Epoch [230/1000], Loss: 0.0062\n",
      "Epoch [240/1000], Loss: 0.0019\n",
      "Epoch [250/1000], Loss: 0.0015\n",
      "Epoch [260/1000], Loss: 0.2436\n",
      "Epoch [270/1000], Loss: 0.2356\n",
      "Epoch [280/1000], Loss: 0.0021\n",
      "Epoch [290/1000], Loss: 0.0037\n",
      "Epoch [300/1000], Loss: 0.0016\n",
      "Epoch [310/1000], Loss: 0.0014\n",
      "Epoch [320/1000], Loss: 0.0004\n",
      "Epoch [330/1000], Loss: 0.3294\n",
      "Epoch [340/1000], Loss: 0.0035\n",
      "Epoch [350/1000], Loss: 0.0008\n",
      "Epoch [360/1000], Loss: 0.0007\n",
      "Epoch [370/1000], Loss: 0.2069\n",
      "Epoch [380/1000], Loss: 0.4842\n",
      "Epoch [390/1000], Loss: 0.0015\n",
      "Epoch [400/1000], Loss: 0.0013\n",
      "Epoch [410/1000], Loss: 0.0352\n",
      "Epoch [420/1000], Loss: 0.0007\n",
      "Epoch [430/1000], Loss: 0.0377\n",
      "Epoch [440/1000], Loss: 0.0000\n",
      "Epoch [450/1000], Loss: 0.0000\n",
      "Epoch [460/1000], Loss: 0.0003\n",
      "Epoch [470/1000], Loss: 0.0001\n",
      "Epoch [480/1000], Loss: 0.2920\n",
      "Epoch [490/1000], Loss: 0.0001\n",
      "Epoch [500/1000], Loss: 0.0000\n",
      "Epoch [510/1000], Loss: 0.0003\n",
      "Epoch [520/1000], Loss: 0.0001\n",
      "Epoch [530/1000], Loss: 0.0004\n",
      "Epoch [540/1000], Loss: 0.2865\n",
      "Epoch [550/1000], Loss: 0.0002\n",
      "Epoch [560/1000], Loss: 0.0261\n",
      "Epoch [570/1000], Loss: 0.0002\n",
      "Epoch [580/1000], Loss: 0.0003\n",
      "Epoch [590/1000], Loss: 0.0004\n",
      "Epoch [600/1000], Loss: 0.0000\n",
      "Epoch [610/1000], Loss: 0.0001\n",
      "Epoch [620/1000], Loss: 0.0003\n",
      "Epoch [630/1000], Loss: 0.0002\n",
      "Epoch [640/1000], Loss: 0.2466\n",
      "Epoch [650/1000], Loss: 0.0000\n",
      "Epoch [660/1000], Loss: 0.0001\n",
      "Epoch [670/1000], Loss: 0.0000\n",
      "Epoch [680/1000], Loss: 0.0001\n",
      "Epoch [690/1000], Loss: 0.0000\n",
      "Epoch [700/1000], Loss: 0.0001\n",
      "Epoch [710/1000], Loss: 0.0000\n",
      "Epoch [720/1000], Loss: 0.0001\n",
      "Epoch [730/1000], Loss: 0.0001\n",
      "Epoch [740/1000], Loss: 0.1957\n",
      "Epoch [750/1000], Loss: 0.0000\n",
      "Epoch [760/1000], Loss: 0.0000\n",
      "Epoch [770/1000], Loss: 0.0000\n",
      "Epoch [780/1000], Loss: 0.0000\n",
      "Epoch [790/1000], Loss: 0.2677\n",
      "Epoch [800/1000], Loss: 0.0000\n",
      "Epoch [810/1000], Loss: 0.0000\n",
      "Epoch [820/1000], Loss: 0.0000\n",
      "Epoch [830/1000], Loss: 0.0002\n",
      "Epoch [840/1000], Loss: 0.0000\n",
      "Epoch [850/1000], Loss: 0.0000\n",
      "Epoch [860/1000], Loss: 0.0000\n",
      "Epoch [870/1000], Loss: 0.0000\n",
      "Epoch [880/1000], Loss: 0.0000\n",
      "Epoch [890/1000], Loss: 0.0000\n",
      "Epoch [900/1000], Loss: 0.0000\n",
      "Epoch [910/1000], Loss: 0.0000\n",
      "Epoch [920/1000], Loss: 0.0000\n",
      "Epoch [930/1000], Loss: 0.2665\n",
      "Epoch [940/1000], Loss: 0.0000\n",
      "Epoch [950/1000], Loss: 0.0000\n",
      "Epoch [960/1000], Loss: 0.2758\n",
      "Epoch [970/1000], Loss: 0.0000\n",
      "Epoch [980/1000], Loss: 0.0000\n",
      "Epoch [990/1000], Loss: 0.0000\n",
      "Epoch [1000/1000], Loss: 0.2845\n",
      "final loss: 0.2845\n",
      "training complete. file saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "\tfor (words,labels) in train_loader:\n",
    "\t\twords = words.to(device)\n",
    "\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t#Forward\n",
    "\t\toutputs = model(words)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\tif (epoch+1) % 10 == 0:\n",
    "\t\tprint(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}\")\n",
    "print(f\"final loss: {loss.item():.4f}\")\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
